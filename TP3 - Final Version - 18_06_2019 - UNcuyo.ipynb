{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author : Baptiste PICARD\n",
    "# email : picard.baptiste@laposte.net\n",
    "# First implementation :26/05/2019\n",
    "# Last Changes : 18/06/2019\n",
    "# Version : V0.4 - Setting model to exercise's consign and documentation\n",
    "# For : IA2 UNcuyo project \n",
    "# Abstract : Short implementation of an MNIST classification (Handwritten's letters).\n",
    "# Links to pages i used : \n",
    "#    - https://github.com/trekhleb/homemade-machine-learning/tree/master/homemade/neural_network\n",
    "#    - https://www.youtube.com/watch?v=W8AeOXa_FqU&app=desktop\n",
    "#    - https://www.youtube.com/watch?v=Dso6nQNGrrw&list=PLjyc6gCk4VU3whCnowAFbVfiBpztCpOKL&index=11\n",
    "#    - https://towardsdatascience.com/how-does-back-propagation-in-artificial-neural-networks-work-c7cad873ea7\n",
    "\n",
    "# Import for the project \n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "from math import exp, tanh\n",
    "import numpy as np\n",
    "import scipy.misc as smp\n",
    "from threading import Thread, RLock\n",
    "\n",
    "# import for the test, with Keras, TensorFlow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "datas = 'A_Z Handwritten Data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes :  0.0009827931722005208 .\n"
     ]
    }
   ],
   "source": [
    "# This function is used to draw an image of an Handwritten letter.\n",
    "# It will transform a list of 785 element (which is an int between 0 = black and 255 = white)\n",
    "# Into a numpy.array() used to draw the handwritten letter.\n",
    "# Input = list python of 285 element.\n",
    "# Output = np.array(28 * 28), image type (use to show)\n",
    "def drawImage(numbers) :\n",
    "    image = smp.toimage(numbers)\n",
    "    image.show()\n",
    "    return image\n",
    "    \n",
    "\n",
    "# This function will prepare the datas for the drawing and the learning. \n",
    "# Input : <Dataframe> which contains all the datas.\n",
    "# Output[1] : Content of the .csv's file shaped to show images 785 -> (1, 28, 28). \n",
    "# Output[2] : Content of the .csv's file shaped to be an entry of the MLP : Dense. \n",
    "# Output[3] : Content of the .csv's file shaped to be an entry of the MLP : Conv2D. \n",
    "def shape_datas(datas) :\n",
    "    start = time.time()\n",
    "    with open(datas, 'r') as file : \n",
    "        datas = csv.reader(file)\n",
    "        results = np.array(np.zeros(784).reshape(1, 28, 28))\n",
    "        entries = np.array(np.zeros(784).reshape(1, 784))\n",
    "        outputs = np.array(np.zeros(26).reshape(1, 26))\n",
    "        limit = 30\n",
    "        state = 0 \n",
    "        for data in datas :\n",
    "            if(state == limit) :\n",
    "                break \n",
    "            else : \n",
    "                state += 1\n",
    "            new_row = np.zeros(784)\n",
    "            new_output = np.zeros(26)\n",
    "            for index_element, element in enumerate(data[1:]) :\n",
    "                new_row[index_element] = float(int(element)/255.0)\n",
    "            new_output[int(new_row[:1])] = 1.0\n",
    "            new_result = new_row.reshape(1, 28, 28)\n",
    "            new_entry = new_row.reshape(1, 784)\n",
    "            new_output = new_output.reshape(1, 26)\n",
    "            results = np.append(results, new_result, axis=0)\n",
    "            entries = np.append(entries, new_entry, axis=0)\n",
    "            outputs = np.append(outputs, new_output, axis=0)\n",
    "    print('It takes : ',(time.time()-start)/60,'.')\n",
    "    return results, entries, outputs  \n",
    "    \n",
    "# This function is used to split the datas into a train's datas and test's datas.\n",
    "# Ratio : 0,75 train, 0,25 test.\n",
    "# Into a numpy.array() used to draw the handwritten letter.\n",
    "# Input = np.array -> inputs and outputs, from the data's csv.\n",
    "# Output = X_train, X_test, Y_train, Y_test.\n",
    "def split_datas(inputs, outputs) :\n",
    "    return train_test_split(inputs, outputs, test_size=0.25)\n",
    "\n",
    "# Create a mlp with keras-tensorflow - 2 capas ocultas, to resolve a classification's problem\n",
    "def getModel_Dense(num_pixels, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(num_pixels,), activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "# Create a convolutionnal model, to resolve a classification's problem\n",
    "def getModel_CV(num_pixels, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(num_pixels, num_pixels, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "\n",
    "#image_shaped, inputs_shaped, outputs_shaped = openCSV_and_shape_datas(datas, 2000)\n",
    "#print(\"X_train[0] dense's model is size  : \",X_train[0].size, \"and type : \",type(X_train[0]))\n",
    "#print(\"X_train[0] conv2D's model is size  : \",X_t[0].size, \"and type : \",type(X_t[0]))\n",
    "#print(\"Y_train[0] is size  : \",Y_train[0].size, \"and type : \",type(X_train[0]))\n",
    "#print(\"-----------------------------------------------------------------------\")\n",
    "#print(\"Type of X_train for dense's model : \",type(X_train))\n",
    "#print(\" and shape : \",X_train.shape, \" and shape of an element : \",X_train[0].shape)\n",
    "#print(\"Type of X_train for Conv2d's model : \",type(X_t))\n",
    "#print(\" and shape : \",X_t.shape, \" and shape of an element : \",X_t[0].shape)\n",
    "#print(\"-----------------------------------------------------------------------\")\n",
    "#print(\"Type of Y : \",type(Y_train))\n",
    "#print(\" and shape : \",Y_train.shape,\" and shape of an element : \",Y_train[0].shape)\n",
    "\n",
    "input_results, input_entries, outputs = shape_datas(datas)\n",
    "X_train, X_test, Y_train, Y_test = split_datas(input_results, outputs)\n",
    "X_t, X_te, Y_t, Y_te = split_datas(input_entries, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Explaining the model ------------------------\n",
      "This model is composed by 2 simples layers.\n",
      "\n",
      "The 1 s layer got : 784 neurons.\n",
      "The activation's function associated is :  tanh .\n",
      "\n",
      "The 2 s layer got : 512 neurons.\n",
      "The activation's function associated is :  tanh .\n",
      "\n",
      "The 3 s layer got : 26 neurons.\n",
      "The activation's function associated is :  tanh .\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Threat : 1 on 22\n",
      "Threat : 2 on 22\n",
      "Threat : 3 on 22\n",
      "Threat : 4 on 22\n",
      "Threat : 5 on 22\n",
      "Threat : 6 on 22\n",
      "Threat : 7 on 22\n",
      "Threat : 8 on 22\n",
      "Threat : 9 on 22\n",
      "Threat : 10 on 22\n",
      "Threat : 11 on 22\n",
      "Threat : 12 on 22\n",
      "Threat : 13 on 22\n",
      "Threat : 14 on 22\n",
      "Threat : 15 on 22\n",
      "Threat : 16 on 22\n",
      "Threat : 17 on 22\n",
      "Threat : 18 on 22\n",
      "Threat : 19 on 22\n",
      "Threat : 20 on 22\n",
      "Threat : 21 on 22\n",
      "Threat : 22 on 22\n",
      "\n",
      "------------ threats terminated ------------\n"
     ]
    }
   ],
   "source": [
    "# Creating the activation function : sigmoid\n",
    "def activation_sigmoid(x) :\n",
    "    return 1 /(1 + exp(-x))\n",
    "\n",
    "# Derivating the activation function : sigmoid\n",
    "def activation_sigmoid_derivated(x) :\n",
    "    #derivada: e^(-x) / (1 + e^(-x))^2\n",
    "    return exp(-x) /(1 + exp(-x))**2\n",
    "\n",
    "# Creating the activation function : tanh\n",
    "def activation_tanh(x) :\n",
    "    return tanh(x)\n",
    "\n",
    "# Derivating the activation function : tanh\n",
    "def activation_tanh_derivated(x) :\n",
    "    return 1-tanh(x)\n",
    "\n",
    "# Creating the activation function : tangent\n",
    "def activation_tangent(x) : \n",
    "    return tan(x)\n",
    "\n",
    "# Derivating the activation function : tangent\n",
    "def activation_tangent_derivated(x) :\n",
    "    return 1 + tan(x)**2\n",
    "\n",
    "# Creating the activation function : softmax\n",
    "def activation_softmax(x) : \n",
    "    return x\n",
    "\n",
    "# Derivating the activation function : softmax\n",
    "def activation_softmax_derivated(x) :\n",
    "    return 1\n",
    "    \n",
    "\n",
    "# class MLP : multi-layer perceptron to learn about datas\n",
    "# Create this class with several parameters\n",
    "class MultilayerPerceptron :\n",
    "    \n",
    "    # Init this class with information.\n",
    "    def __init__(self, name, datas, layers, learning_rate=0.30, activation=['tanh', 'tanh', 'tanh']) : \n",
    "        self.name = name # Name of the MLP, String \n",
    "        self.datas = datas # Datas to train the model, np.array([X_train, Y_train, X_test, Y_test])\n",
    "        self.layers = layers # Define the number of layers of the MLP and the number of neurons on each layer, np.array([784, 512, 26])\n",
    "        self.learning_rate = learning_rate # Learning rate : 0.30\n",
    "        self.activation = activation # Define the activation's function for each layer : np.array(['sigmoid', 'sigmoid', 'softmax'])\n",
    "        self.weights = self.create_model_weights() # Neuron's weights of the model \n",
    "        self.values = self.create_model_values() # Neuron's values of the model \n",
    "        self.new_values = self.create_model_values() # Copy of self.values, use to back_propagated the error\n",
    "        self.old_weights = self.create_model_weights() # Copy of self.weights, use to back-propagate the error \n",
    "    \n",
    "    # Getter and Setter of the variables of the model.\n",
    "    # ------------------------------------------------\n",
    "    def get_name(self) : # Return the name of the MLP\n",
    "        return self.name \n",
    "    \n",
    "    def get_layers(self) : # Return a list of the 3 layers and the number of neurons of each one \n",
    "        return self.layers\n",
    "    \n",
    "    def get_layer_index(self, index) : # Return an integer corresponding to the number of neurons of the layer : index\n",
    "        return self.layers[index]\n",
    "    \n",
    "    def get_X_train(self) : # Return X_train\n",
    "        return self.datas[0]\n",
    "    \n",
    "    def get_Y_train(self) : # Return Y_train \n",
    "        return self.datas[1]\n",
    "    \n",
    "    def get_X_test(self) : # Return X_test\n",
    "        return self.datas[2] \n",
    "    \n",
    "    def get_Y_test(self) : # Return Y_test\n",
    "        return self.datas[3]\n",
    "    \n",
    "    def get_learning_rate(self) : # Return the learning rate \n",
    "        return self.learning_rate\n",
    "    \n",
    "    def set_learning_rate(self, new_learning_rate) : # Set the learning rate to a new value \n",
    "        if(new_learning_rate>=0.0 and new_learning_rate<=1.0) :\n",
    "            self.learning_rate = new_learning_rate\n",
    "    \n",
    "    def get_activations(self) : # Return the list with the correspondings activation functions of each layer of MLP.\n",
    "        return self.activation\n",
    "    \n",
    "    def get_activation_layer(self, layer) : # Return the activation function (String) corresponding to the layer : layer \n",
    "        return self.activation[layer]\n",
    "    \n",
    "        # For the variable : weights \n",
    "    # -----------------------\n",
    "    def get_model_weights(self) : # Return all the weights (np.array) of the model.\n",
    "        return self.weights\n",
    "    \n",
    "    def get_model_weights_layer(self, layer, index_value_associated) : # Return a layer of the weights associated to a specific value \n",
    "        return self.weights[layer][index_value_associated]\n",
    "    \n",
    "    def get_weight_neuron(self, x, y, z) : # Get a weight of a neuron in self.weights\n",
    "        return self.weights[x][y][z]\n",
    "    \n",
    "    def set_weight_neuron(self, x, y, z, value) : # Set the weight of a neuron -> in self.weghts\n",
    "        if(value >= 0.0 and value <= 1.0) :\n",
    "            self.weights[x][y][z] = value\n",
    "            \n",
    "    def set_model_weights_layer(self, layer, index, new_layer) : # Set the weights associated to a value (np.array())\n",
    "        self.weights[layer][index] = new_layer\n",
    "    \n",
    "        # For the variable : values \n",
    "    # -----------------------\n",
    "    def get_model_values(self) : # Return all the values (np.array) of the model\n",
    "        return self.values\n",
    "    \n",
    "    def get_model_values_layer(self, layer) : # Return a layer of the values\n",
    "        return self.values[layer]\n",
    "            \n",
    "    def set_model_values_layer(self, layer, new_layer) : # Set a layer of values (np.array())\n",
    "        self.values[layer] = new_layer\n",
    "    \n",
    "    def get_value_neuron(self, x, y) : # Get a value of a neuron in self.values\n",
    "        return self.values[x][y]\n",
    "    \n",
    "    def set_value_neuron(self, x, y, value) : # Set the value of a neuron -> in self.values\n",
    "        if(value >= 0.0 and value <= 1.0) :\n",
    "            self.values[x][y] = value\n",
    "    \n",
    "        # For the variable : new_values \n",
    "    # -----------------------\n",
    "    def get_model_new_values(self) : # Return all the new_values (np.array) of the model -> used for the back-propagation\n",
    "        return self.new_values\n",
    "\n",
    "    def get_new_value_neuron(self, x, y) : # Get a value of a neuron in self.new_values\n",
    "        return self.new_values[x][y]\n",
    "    \n",
    "    def set_new_values_neuron(self, x, y, value) : # Set an item of new_values to value\n",
    "        self.new_values[x][y] = value \n",
    "        \n",
    "        # For the variable : old_weights \n",
    "    # -----------------------\n",
    "    def get_model_old_weights(self) : # Return all the old_weights (np.array) of the model -> used for the back-propagation\n",
    "        return self.old_weights\n",
    "    \n",
    "    def get_old_weight_neuron(self, x, y, z) : # Get a the old_weight of a neuron in self.old_weights\n",
    "        return self.old_weights[x][y][z]\n",
    "    \n",
    "    def set_old_weights_neuron(self, x, y, z, value) : # Set an item of new_values to value\n",
    "        self.old_weights[x][y][z] = value \n",
    "        \n",
    "        # Functions to initialise the weights and values.\n",
    "    # ------------------------------------------------\n",
    "    def create_model_weights(self) : # Create the weights of the model, each weight is initialise between [-1.0 : 1.0]\n",
    "        init_model = []\n",
    "        for index_layer,layer in enumerate(self.get_layers()[:-1]) :\n",
    "            new_weights = []\n",
    "            for i in range(self.get_layer_index(index_layer+1)) :\n",
    "                new = np.zeros(layer)\n",
    "                for index in range (new.size) :\n",
    "                    new[index] = random.uniform(-1.0, 1.0)\n",
    "                new_weights.append(new)\n",
    "            init_model.append(np.array(new_weights))\n",
    "        return np.array(init_model)\n",
    "            \n",
    "    def create_model_values(self) : # Create the values of the model, each value is initialise between [0.0 : 1.0]\n",
    "        init_model = []\n",
    "        for index, layer in enumerate(self.get_layers()) : \n",
    "            if(index==0) :\n",
    "                init_model.append(np.zeros(layer))\n",
    "            else : \n",
    "                new_layer=np.zeros(layer)\n",
    "                for i in range(layer) :\n",
    "                    new_layer[i] = random.random()\n",
    "                init_model.append(new_layer) \n",
    "        return np.array(init_model)\n",
    "        \n",
    "    def copy_values_to_new_values(self) : # Copy all the float of values into new_values\n",
    "        for index_layer, layer in enumerate(self.get_model_values()) :\n",
    "            for index_value, value in enumerate(layer) :\n",
    "                self.set_new_values_neuron(index_layer, index_value, value) \n",
    "                \n",
    "    def copy_weights_to_old_weights(self) : # Copy all the float of weights into weights\n",
    "        for index_layer, layer in enumerate(self.get_model_weights()) :\n",
    "            for index_value, value in enumerate(layer) :\n",
    "                for index_item, item in enumerate(value) :\n",
    "                    self.set_old_weights_neuron(index_layer, index_value, index_item, item) \n",
    "        \n",
    "        # Functions to activate a float with a function or its derivated.\n",
    "    # ------------------------------------------------        \n",
    "    def activate(self, function, value) :\n",
    "        # Using a value (float) and a function (String), return the result of the value in the function\n",
    "        if(function == 'sigmoid' ) :\n",
    "            return activation_sigmoid(value)\n",
    "        elif(function == 'tangent' ) :\n",
    "            return activation_tangent(value)\n",
    "        elif(function == \"softmax\") :\n",
    "            return activation_softmax(value)\n",
    "        elif(function=='tanh') :\n",
    "            return activation_tanh(value)\n",
    "        \n",
    "    def activate_derivated(self, function, value) : \n",
    "        # Using a value (float) and a function (String), return the result of the value in the derivated's function\n",
    "        if(function == 'sigmoid' ) :\n",
    "            return activation_sigmoid_derivated(value)\n",
    "        elif(function == 'tangent' ) :\n",
    "            return activation_tangent_derivated(value)\n",
    "        elif(function == \"softmax\") :\n",
    "            return activation_softmax_derivated(value)\n",
    "        elif(function == 'tanh') :\n",
    "            return activation_tanh_derivated(value)\n",
    "        \n",
    "        # Functions to propagate the value, backpropagate the error or to show the model.\n",
    "    # ------------------------------------------------     \n",
    "    def show_model(self) : # Show the model -> used to make a summary of the MLP\n",
    "        print(\"--------------------- Explaining the model ------------------------\")\n",
    "        print(\"This model is composed by\", self.get_model_weights().size, \"simples layers.\\n\")\n",
    "        for index, layer in enumerate(self.get_layers()) :\n",
    "            print('The',index+1,\"s layer got :\",layer,\"neurons.\")\n",
    "            print(\"The activation's function associated is : \",self.activation[index],'.\\n')\n",
    "        print(\"-------------------------------------------------------------------\")\n",
    "    \n",
    "    def test_model_and_show_error(self) : # Use to test and recuperate different errors\n",
    "        pass\n",
    "    \n",
    "    def propagation(self, X) : \n",
    "        # Using an entry (of X_train) -> We propagate the entry to the output layer.\n",
    "        # To propagate the entry to the output, we use the weights of a value and the values of the layers.\n",
    "        # The next_value = activation(sum(value_i * weight_i) - actual_value)\n",
    "        # test size X_train -> weights.size at inputs\n",
    "        if(X.size == self.get_model_values()[0].size) : # Test the entry size : X_train <-> model\n",
    "            # For each layer of the MLP -> 3 layers : 0, 1, 2\n",
    "            for index_layer in range(self.get_layers().size) : \n",
    "                \n",
    "                # The first layer correspond to the values o X_train.\n",
    "                # We simply copy the entry'values into values\n",
    "                if(index_layer == 0) : \n",
    "                    self.set_model_values_layer(0, X) # Set the value's layers with X_train's values\n",
    "                    self.copy_values_to_new_values() # Copy actual values to new values\n",
    "                    \n",
    "                # Others layers need propagation using first layer.\n",
    "                # Using the first layer (train values), we propagate the values to the next layer using the weights,\n",
    "                # The old values, and the activation's functions.\n",
    "                # This is the first step, which helps to predict the output using various entries.\n",
    "                # With the propagation, we can see if we need to set the weight between layers and the values of the neurons \n",
    "                # Or if we can just keep or model.\n",
    "                else : \n",
    "                    # For each value in the next layer [1, 2] -> the layer 0 was call before\n",
    "                    for index_value, value in enumerate(self.get_model_values_layer(index_layer)) :\n",
    "                        new_val = 0.0 # New value by propagation  \n",
    "                        # Propagation's equation we will use after : \n",
    "                        # To calculate the next value of a neuron with the neurons of the previous layer, \n",
    "                        # new_val = activation_function( sum(previous_neuron_i * weight_previous_to_next_neuron) - actual_value ).\n",
    "                        for index_previous_value, previous_value in enumerate(self.get_model_values_layer(index_layer-1)) : \n",
    "                            new_val = new_val + previous_value * self.get_weight_neuron(index_layer-1, index_value, index_previous_value) \n",
    "                        new_val = self.activate(self.get_activation_layer(index_layer), new_val - self.get_value_neuron(index_layer, index_value)) # Activating with the right function\n",
    "                        self.set_value_neuron(index_layer, index_value, new_val) # Set the value to the value propagated\n",
    "        else : \n",
    "            print('Error in shaping entries.')\n",
    "            print('X : ',X.size,' and : ',self.get_model_values_layer(0).size)\n",
    "            \n",
    "        \n",
    "    def backpropagation(self, out_layer):\n",
    "        delta_d = np.zeros(self.get_model_values_layer(2).size) # creating the deltas, using to give the new value/weight of a layer/neuron.    \n",
    "        self.copy_weights_to_old_weights() # Creating a copy of the weigths.\n",
    "        if(out_layer.size == self.get_model_values_layer(2).size) : # test size X_train -> weights.size at inputs\n",
    "            \n",
    "            # For each layer of the MLP (3 layer) -> Layers : 2, 1, 0\n",
    "            for index_layer in range(self.get_layers().size-1,0,-1) : \n",
    "                # Each neuron of the layer\n",
    "                for index_value, value in enumerate(self.get_model_values_layer(index_layer)) :\n",
    "                    \n",
    "                    # We calculate the h_d, in the same way of the porpagation of a value, but we use the activation's function derivated\n",
    "                    # To activate the value. The variable 'h_d' was made to propagate the error, we use it to each delta\n",
    "                    # Of each layer.\n",
    "                    h_ = 0.0 \n",
    "                    for index_previous_value, previous_value in enumerate(self.get_model_values_layer(index_layer-1)) :\n",
    "                        h_ += previous_value * self.get_weight_neuron(index_layer-1, index_value, index_previous_value)\n",
    "                    h_ += - self.new_values[index_layer][index_value]\n",
    "                    h_d = self.activate_derivated('tanh', h_) # h_d = activation_derivated(sum(w_i * v_i) - value_anterior)\n",
    "                    \n",
    "                    # If are of the last layer, we need to create the delta, using the difference between real and predicted output \n",
    "                    if (index_layer==2) : \n",
    "                        delta_d[index_value] = h_d*(out_layer[index_value] - self.get_value_neuron(index_layer, index_value)) # Creating the delta according to the neuron.\n",
    "                        seters = np.zeros(self.get_model_values_layer(index_layer-1).size+1) # To save the variation of the weight/value's neuron\n",
    "                        actual_w_v = np.zeros(self.get_model_values_layer(index_layer-1).size+1) # To get the weight/value's neuron\n",
    "                        # For each item of the np.array(), we calculate the variation and save it \n",
    "                        for index_seter, seter in enumerate(seters) : \n",
    "                            # Corresponding to a value \n",
    "                            if(index_seter == 0) : \n",
    "                                seters[index_seter] = self.get_learning_rate() * delta_d[index_value] * out_layer[index_value] # Calculate the variation \n",
    "                                actual_w_v[index_seter] = self.get_new_value_neuron(index_layer, index_value) # Get the right value \n",
    "                            # Corresponding to weights\n",
    "                            else : \n",
    "                                seters[index_seter] = self.get_learning_rate() * delta_d[index_value] * self.get_value_neuron(index_layer-1, index_seter-1) # Calculate the variation\n",
    "                                actual_w_v[index_seter] = self.get_weight_neuron(index_layer-1, index_value, index_seter-1) # Get the right Weight\n",
    "                        news = seters + actual_w_v # Get the new value and the news weights \n",
    "                    \n",
    "                    # If we are not in the last layer, we don't calculate the delta with the same equation.\n",
    "                    else : \n",
    "                        delta_d_1 = 0.0\n",
    "                        # Caltulate the delta for each neuron of the hidden layer, we need the delta of the output layer\n",
    "                        # delta = sum(output_neuron_delta * weight) * h_d\n",
    "                        for index_delta, delta in enumerate(delta_d) :\n",
    "                            delta_d_1 += delta * self.old_weights[index_layer][0][index_delta]\n",
    "                        delta_d_1 = delta_d_1 * h_d # We calculate the new delta with the deltas of the output layer.\n",
    "                        seters = np.zeros(self.get_model_values_layer(index_layer-1).size+1) # To save the variation of the weight/value's neuron\n",
    "                        actual_w_v = np.zeros(self.get_model_values_layer(index_layer-1).size+1) # To get the weight/value's neuron\n",
    "                        # For each item of the np.array(), we calculate the variation and save it\n",
    "                        for index_seter, seter in enumerate(seters) : \n",
    "                            # Corresponding to a value\n",
    "                            if(index_seter == 0) :\n",
    "                                seters[index_seter] = self.get_learning_rate() * delta_d_1 * (-1) # Calculate the variation \n",
    "                                actual_w_v[index_seter] = self.get_new_value_neuron(index_layer, index_value) # Get the right value \n",
    "                            # Corresponding to weights\n",
    "                            else :\n",
    "                                seters[index_seter] = self.get_learning_rate() * delta_d_1 * self.get_value_neuron(index_layer-1, index_seter-1) # Calculate the variation\n",
    "                                actual_w_v[index_seter] = self.get_weight_neuron(index_layer-1, index_value, index_seter-1) # Get the right Weight\n",
    "                        news = seters + actual_w_v # Get the new value and the news weights      \n",
    "                       \n",
    "                    # When we finished to calculate the new values, we need to set the old value/weight by the new value/weight  \n",
    "                    # For all items in 'new', we enulerate them\n",
    "                    for index_new, new in enumerate(news) : \n",
    "                        # index = 0, it correspond to a value \n",
    "                        if(index_new == 0) :   \n",
    "                            self.new_values[index_layer][index_value] = news[index_new] # We set the value of the right neuron by it new value\n",
    "                        # index = not(0) , it correspond to a weight\n",
    "                        else :   \n",
    "                            self.set_weight_neuron(index_layer-1, index_value, index_new-1, news[index_new]) # We set the weight of the right neuron by it new weight\n",
    "            \n",
    "        else : \n",
    "            print('Error in shaping outputs.')\n",
    "            print('Y : ',out_layer,' and : ',self.get_model_values_layer(2))\n",
    "        \n",
    "    def calculate_rendimiento(self) : \n",
    "        loss = 0.0\n",
    "        accuracy = 0.0\n",
    "        print(\"Loss : {0}, Accuracy : {1}\".format(loss, accuracy))\n",
    "        return loss, accuracy\n",
    "            \n",
    "X_t = X_t.reshape(23, 784)\n",
    "mlp = MultilayerPerceptron(\"First_MLP\", [X_t, Y_t, X_test, Y_test], np.array([784, 512, 26]), 0.3) # Creating a model, Multi Layer Perceptron\n",
    "mlp.show_model() # Show informations\n",
    "for i in range(0, len(mlp.get_X_train())-1) :\n",
    "    print(\"Threat : {0} on {1}\".format(i+1, len(mlp.get_X_train())-1))\n",
    "    mlp.propagation(mlp.get_X_train()[i])\n",
    "    mlp.backpropagation(mlp.get_Y_train()[i])\n",
    "print(\"\\n------------ threats terminated ------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23 samples, validate on 8 samples\n",
      "Epoch 1/30\n",
      " - 2s - loss: 3.0910 - acc: 0.0435 - val_loss: 3.1475 - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      " - 0s - loss: 3.0294 - acc: 0.3478 - val_loss: 3.0692 - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      " - 0s - loss: 2.9617 - acc: 0.7391 - val_loss: 2.9718 - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      " - 0s - loss: 2.8717 - acc: 0.7391 - val_loss: 2.8513 - val_acc: 1.0000\n",
      "Epoch 5/30\n",
      " - 0s - loss: 2.7732 - acc: 0.8261 - val_loss: 2.7012 - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      " - 0s - loss: 2.6221 - acc: 0.9130 - val_loss: 2.5176 - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      " - 0s - loss: 2.5103 - acc: 0.9565 - val_loss: 2.2942 - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      " - 0s - loss: 2.2872 - acc: 0.9565 - val_loss: 2.0237 - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      " - 0s - loss: 2.0849 - acc: 0.9565 - val_loss: 1.7019 - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      " - 0s - loss: 1.8307 - acc: 1.0000 - val_loss: 1.3336 - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      " - 0s - loss: 1.3579 - acc: 1.0000 - val_loss: 0.9438 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      " - 0s - loss: 1.3007 - acc: 1.0000 - val_loss: 0.5766 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.9060 - acc: 1.0000 - val_loss: 0.2910 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.5657 - acc: 1.0000 - val_loss: 0.1194 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.3942 - acc: 1.0000 - val_loss: 0.0403 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.2108 - acc: 1.0000 - val_loss: 0.0117 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.1126 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.0562 - acc: 1.0000 - val_loss: 7.2251e-04 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.0586 - acc: 1.0000 - val_loss: 1.6263e-04 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.0485 - acc: 1.0000 - val_loss: 3.4870e-05 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.0096 - acc: 1.0000 - val_loss: 7.1601e-06 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.0075 - acc: 1.0000 - val_loss: 1.4603e-06 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.0368 - acc: 1.0000 - val_loss: 3.1292e-07 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.0082 - acc: 1.0000 - val_loss: 1.2666e-07 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      " - 0s - loss: 7.2288e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      " - 0s - loss: 3.0919e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      " - 0s - loss: 1.6909e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      " - 0s - loss: 1.0112e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      " - 0s - loss: 9.2897e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      " - 0s - loss: 3.2517e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Loss :  0 % / Accuracy :  100 %.\n"
     ]
    }
   ],
   "source": [
    "model_2d = getModel_CV(28, 26)\n",
    "X_t = X_t.reshape(23, 28, 28, 1)\n",
    "X_te = X_te.reshape(8, 28, 28, 1)\n",
    "model_2d.fit(X_t, Y_t, validation_data=(X_te, Y_te), epochs=30, batch_size=50, verbose=2)\n",
    "scores = model_2d.evaluate(X_te, Y_te, verbose=0)\n",
    "print(\"Loss : \",int(scores[0]*100),\"% / Accuracy : \",int(scores[1]*100),\"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 26)                13338     \n",
      "=================================================================\n",
      "Total params: 677,914\n",
      "Trainable params: 677,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 23 samples, validate on 8 samples\n",
      "Epoch 1/15\n",
      " - 1s - loss: 3.2033 - acc: 0.0000e+00 - val_loss: 0.0190 - val_acc: 1.0000\n",
      "Epoch 2/15\n",
      " - 0s - loss: 0.0698 - acc: 1.0000 - val_loss: 4.6803e-04 - val_acc: 1.0000\n",
      "Epoch 3/15\n",
      " - 0s - loss: 0.0061 - acc: 1.0000 - val_loss: 1.9170e-04 - val_acc: 1.0000\n",
      "Epoch 4/15\n",
      " - 0s - loss: 0.0034 - acc: 1.0000 - val_loss: 9.5674e-05 - val_acc: 1.0000\n",
      "Epoch 5/15\n",
      " - 0s - loss: 0.0021 - acc: 1.0000 - val_loss: 5.3856e-05 - val_acc: 1.0000\n",
      "Epoch 6/15\n",
      " - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 3.3037e-05 - val_acc: 1.0000\n",
      "Epoch 7/15\n",
      " - 0s - loss: 0.0010 - acc: 1.0000 - val_loss: 2.1585e-05 - val_acc: 1.0000\n",
      "Epoch 8/15\n",
      " - 0s - loss: 7.8135e-04 - acc: 1.0000 - val_loss: 1.4760e-05 - val_acc: 1.0000\n",
      "Epoch 9/15\n",
      " - 0s - loss: 6.0424e-04 - acc: 1.0000 - val_loss: 1.0610e-05 - val_acc: 1.0000\n",
      "Epoch 10/15\n",
      " - 0s - loss: 4.8153e-04 - acc: 1.0000 - val_loss: 7.8530e-06 - val_acc: 1.0000\n",
      "Epoch 11/15\n",
      " - 0s - loss: 3.9315e-04 - acc: 1.0000 - val_loss: 5.9456e-06 - val_acc: 1.0000\n",
      "Epoch 12/15\n",
      " - 0s - loss: 3.2630e-04 - acc: 1.0000 - val_loss: 4.6045e-06 - val_acc: 1.0000\n",
      "Epoch 13/15\n",
      " - 0s - loss: 2.7510e-04 - acc: 1.0000 - val_loss: 3.6210e-06 - val_acc: 1.0000\n",
      "Epoch 14/15\n",
      " - 0s - loss: 2.3500e-04 - acc: 1.0000 - val_loss: 2.9132e-06 - val_acc: 1.0000\n",
      "Epoch 15/15\n",
      " - 0s - loss: 2.0256e-04 - acc: 1.0000 - val_loss: 2.3991e-06 - val_acc: 1.0000\n",
      "Loss :  0 % / Accuracy :  100 %.\n"
     ]
    }
   ],
   "source": [
    "model = getModel_Dense(784,26)\n",
    "model.summary()\n",
    "X_train = X_train.reshape(23, 784)\n",
    "X_test = X_test.reshape(8, 784)\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=15, batch_size=50, verbose=2)\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Loss : \",int(scores[0]*100),\"% / Accuracy : \",int(scores[1]*100),\"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
